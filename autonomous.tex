\chapter{Autonomous Load Balancing}
\label{chapter:auto-balance}


%TODO CITE BELOW FOR NON UNIFORM DISTRBUTION

%http://michiel.buddingh.eu/distribution-of-hash-values


The following analysis and simulation relies on an important assumption about DHT behavior often assumed but not necessarily implemented.
We assume that nodes are active and aggressive in creating and monitoring the backups and the data they are responsible for.
Specifically, we will assume  it takes  $T_{detect}$ time for a node to detect a change in their responsibility or to detect a new node to hand a backup to and that this check is performed regularly.
Another assumption is that nodes do note have control in choosing their IDs from the range of hash values.

Smaller chunking results in more files spread throughout the  network and a greater chance of the data being evenly spread across the network 

The chances of a critical failure happening within a time interval $ T $ is the chances of some chain or cluster of nodes responsible for a single record dying within $ T $:

$$r^{s}$$

Where $ r $ is the failure rate over that time interval and $s$ is the number nodes storing that record, either as a primary system, or a backup.
Incidentally, this time interval $T = T_{detect} + T_{transfer} $


\section{The Simulation}

We simulate an UrDHT Voronoi based-network in multiple dimensions. \footnote{UrDHT in one dimension is a Chord ring with the definition of responsibility changed to a node being responsible to all data closest to it. A 2-dimensional network will emulate the performance of CAN.}

We assume that the network starts our experiments stable and the data necessary already present on the nodes and backed-up.

\subsection{The Parameters}

\subsubsection{Constants}

\begin{description}
	\item [Time Unit] In a simulation, normal measurements of time such as a second are arbitrary, so we be using the abstract \textit{tick} to measure time.  
	If we want to be more concrete, a tick is the amount of time it takes a node to complete one task per sybil and perform the appropriate maintanence.\footnote{The shortest unit of time in the multiverse is the New York Second, defined as the period of time between the traffic lights turning green and the cab behind you honking.\\-- Terry Pratchett}
	\item [Maintence] We assume the reactive, aggressive backups works.
	\item [Hash Functions] We will be using SHA-1 \cite{sha1}, a 160-bit hash function.
\end{description}

\subsubsection{Experimental Variables}
\begin{description}
	\item [Churn] Measured in ticks, this can be self induced or a result of actual turbulence in the network.
	Like most analyses of churn \cite{marozzo2012p2p}, we assume churn is constant throughout the experiment and that the joining and leaving rate are equal.
	\item [Network Size]  How many nodes start in the network.  
		We assume that this can grow during the experiment, either via churn or by creating sybils.
	\item [Pool Size]  The size of the pool of potentially joining nodes.  Each tick, each node in the pool has a chance to join the network, dictated by the churn rate.
		Talk about relative equalibrium
	\item [Size of the job] The size of the job, in tasks.
		This number is typically orders of maginitude greater than the network size.
\end{description}



\section{Baseline Readings}

\section{Induced Churn}
In this experiment, we rely solely on churn to perform load balancing.
We use the assumption that 


\section{Random Sybil Injection}
Our second series of experiments focused on nodes with low amounts of work performing a controlled and strategic Sybil attack \cite{sybil} on the network.
In this experiment, each once each node's workload was at or below a certain threshold, the node would attempt to acquire more work by creating virtual Sybil nodes at random addresses.




No benefit was shown by increasing maxSybils beyond 10, so we stopped increasing it there.


